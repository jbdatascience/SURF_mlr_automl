---
title: "Automatic Machine Learning"
author: Jose Magana
date: 2 Nov 2017
output:
  revealjs::revealjs_presentation:
    theme: black
    highlight: zenburn
    center: true
    transition: slide
    incremental: false
    css: styles.css
    reveal_options:
      width: 1200
      height: 900
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=F}
options(width = 200)
```

# WHAT IS AUTOMATIC MACHINE LEARNING

## What is the purpose

- Search configuration that provides the more effective predictive models.
- Require less code to generate those models.
- Allow quick iteration given different problem frames.
- Free time dedicated to routine low value tasks ( 2 out of [the 4D's](https://www.forbes.com/sites/bernardmarr/2017/10/16/the-4-ds-of-robotization-dull-dirty-dangerous-and-dear): Dull and Dirty)
- Hot research topic
- Support Data Scientists, not replace them. 
- Automatic Machine Learning is NOT Automatic Data Science,
- <b>To date</b>, there is no production level algorithm that can translate a business need expressed in natural language to an algorithm.

<img src=./automl-figure/computers-algorithm-big_data-data_science-data_analytics-job_loss-jcen1731_low.jpg
 height=300>

## What can Automatic Machine Learning do

- <b>Exploratory data analysis</b>
- <b>Data cleaning:</b> missing values, outliers
- <b>Feature selection:</b> redundant, not informative
- <b>Feature engineering</b>
- <b>Data separation</b>: Training / Validation / Test
- <b>Model selection</b>
- <b>Model composition:</b> ensembling, stacking
- <b>Hyperparameter optimization</b>
- <b>Model diagnose</b>

## {data-background-image="./automl-figure/tpot-ml-pipeline.png" data-background-size="100% 100%" data-background-transition="slide"}

# Existing solutions 

## Open Source 

## Auto sklearn

- Python, scikit-learn framework
- For each of 140 datasets (training set), 38 meta-features are calculated on data characteristics: sample size, skewness, classes, entropy, ...
- Run SMAC, to find the best ML model for each
- The distance of a new dataset to this 140 datasets will be measured.
- The ML solutions of the 25 closest are used in a Bayesian Optimization to find the best instantiation
- Model ensemble
- Winner of the [ChaLearn AutoML challenge](http://automl.chalearn.org/) 
- [NIPS Paper](http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf)

<img src=automl-figure/auto-sklearn-overview.jpg height=300>

## TPOT

- [Tree-based Pipeline Optimization Tool](http://automl.info/tpot/) 
- Python, scikit-learn framework
- Combining pipelines expressed as trees
- Based on Genetic Algorithms

<img src=automl-figure/tpot-pipeline-example-768x361.png height=500>

## Other

- [A repository of auto ML and related solutions](https://github.com/automl)
- [AUTOWEKA](http://www.cs.ubc.ca/labs/beta/Projects/autoweka/)(Java/GUI)
- [Sequential Model-based Algorithm Configuration(SMAC)](https://github.com/automl/SMAC3): Used by autoML for Hyperparameter optimization

## Assisted Data Cleaning: OpenRefine(by Google)

<img src=automl-figure/OpenRefine.PNG height=600>

## Commercial (Far from exhaustive)

- [Data Robot](https://www.datarobot.com/) 
<img src=automl-figure/datarobot.png height=250>
- [Skytree](http://www.skytree.net/products/) 
<img src=automl-figure/skytree.png height=250>
- [Automatic Statistician](https://www.automaticstatistician.com) (for Exploratory Data Analysis)
 
 Examples: [Airline](https://www.automaticstatistician.com/static/abcdoutput/01-airline.pdf)
          & [Affairs](file:///auto-report-affairs.pdf)
          
## Towards Skynet: Google AutoML

<img src=automl-figure/googleMLlarge.PNG height=300>
<img src=automl-figure/learning.PNG height=200>

Source:[Wired](https://www.wired.com/story/googles-learning-software-learns-to-write-learning-software/)
Image:[themerkle](https://themerkle.com/googles-automl-ai-clones-itself-into-a-better-version/)

## Automatic Story Telling

There are solutions that <b>claim</b> can convert data to personalised stories around that data in natural language. An example is: [<b>Narrative Science</b>](https://narrativescience.com)

  <img alt="Data" src=automl-figure/narrativescience.PNG height=400>
  
  Store 9, your sales of item 6 are far below other stores in your region. If you are able to up your sales by only 5 units a day, you will be able to increase your profits by $1,123. The sales of this product for other stores in your region seem to indicate that this is completely achievable.

# R packages for Automatic Machine Learning

## The main packages

- [H2O](https://www.h2o.ai/h2o/) (also Python, Java and Scala)
- Caret 
- MLR
- Do you know any other?

## An H2O example: the code

```{r, eval=FALSE, class.source='stretch'}
library(h2o)

h2o.init()

# Import a sample binary outcome train/test set into H2O
train <- h2o.importFile("https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv")
test <- h2o.importFile("https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv")

# Identify predictors and response
y <- "response"
x <- setdiff(names(train), y)

# For binary classification, response should be a factor
train[,y] <- as.factor(train[,y])
test[,y] <- as.factor(test[,y])

aml <- h2o.automl(x = x, y = y,
                  training_frame = train,
                  leaderboard_frame = test,
                  max_runtime_secs = 100)  # was 30

# If you need to generate predictions on a test set, you can make predictions 
# directly on the `"H2OAutoML"` object, or on the leader model object directly
pred <- h2o.predict(aml@leader, test)

```

## An H20 example: the top results

```{r, eval=FALSE, class.source='stretch'}
# View the AutoML Leaderboard
 aml@leaderboard
#                                             model_id      auc  logloss
# 1           StackedEnsemble_model_1494643945817_1709 0.780384 0.561501
# 2 GBM_grid__95ebce3d26cd9d3997a3149454984550_model_0 0.764791 0.664823
# 3 GBM_grid__95ebce3d26cd9d3997a3149454984550_model_2 0.758109 0.593887
# 4                          DRF_model_1494643945817_3 0.736786 0.614430
# 5                        XRT_model_1494643945817_461 0.735946 0.602142
# 6 GBM_grid__95ebce3d26cd9d3997a3149454984550_model_3 0.729492 0.667036
# 7 GBM_grid__95ebce3d26cd9d3997a3149454984550_model_1 0.727456 0.675624
# 8 GLM_grid__95ebce3d26cd9d3997a3149454984550_model_1 0.685216 0.635137
# 9 GLM_grid__95ebce3d26cd9d3997a3149454984550_model_0 0.685216 0.635137
```

## An H20 example: the best result

```{r, eval=FALSE, class.source='stretch'}
# The leader model is stored here
aml@leader
# Model Details:
# ==============
# 
# H2OBinomialModel: stackedensemble
# Model ID:  StackedEnsemble_0_AutoML_20171019_000411 
# NULL
# 
# H2OBinomialMetrics: stackedensemble
# ** Reported on training data. **
# 
# MSE:  0.110144
# RMSE:  0.3318794
# LogLoss:  0.3738966
# Mean Per-Class Error:  0.133218
# AUC:  0.9485615
# Gini:  0.8971229
# 
# Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
#           0    1    Error       Rate
# 0      2658  645 0.195277  =645/3303
# 1       264 3446 0.071159  =264/3710
# Totals 2922 4091 0.129616  =909/7013
# 
# Maximum Metrics: Maximum metrics at their respective thresholds
#                         metric threshold    value idx
# 1                       max f1  0.457365 0.883476 223
# 2                       max f2  0.339376 0.925794 270
# 3                 max f0point5  0.620646 0.891058 155
# 4                 max accuracy  0.468095 0.871097 219
# 5                max precision  0.943729 1.000000   0
# 6                   max recall  0.170628 1.000000 348
# 7              max specificity  0.943729 1.000000   0
# 8             max absolute_mcc  0.468095 0.743003 219
# 9   max min_per_class_accuracy  0.529326 0.865577 193
# 10 max mean_per_class_accuracy  0.506534 0.869358 203
# 
# Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or 
#  `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`

```

Source: (H20)[http://h2o-release.s3.amazonaws.com/h2o/master/3888/docs-website/h2o-docs/automl.html#code-examples]

## CARET

1. Short for <b>C</b>lassification <b>A</b>nd <b>RE</b>gression <b>T</b>raining.
2. Documented extensively: [book Applied Predictive Modelling](http://www.springer.com/gp/book/9781461468486), a [paper](https://www.jstatsoft.org/article/view/v028i05) and a [Datacamp training](https://www.datacamp.com/courses/machine-learning-toolbox).
3. Focused on less steps of the processing pipeline:
a. data splitting
b. pre-processing
c. feature selection
d. model tuning using resampling
e. variable importance estimation

## MLR

- Covers the whole ML pipeline
- Extensible 
- Built-in parallelization
- [ShinyApp available](https://github.com/mlr-org/shinyMlr), the top of automation, right?
- Documentation: [MLR package](https://cran.r-project.org/web/packages/mlr/index.html), [Tutorials](https://mlr-org.github.io/mlr-tutorial/devel/html/) 

# End of Part I

## License

Copyright 2017 Jose A. Magana Mesa

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

## What is Hyper Parameter Model Based Optimization

<img src=./automl-figure/animation-.gif height=600>


