---
title: "mlr"
author: Jose Magana
output: html_notebook
---



```{r}
library(mlr)
library(ggplot2)
library(dplyr)
library(parallelMap)
library(data.table)
```

# Load Data

```{r}
train <- read.csv("train_loan.csv", na.strings = c(""," ",NA))
test <- read.csv("test_Y3wMUE5.csv", na.strings = c(""," ",NA))

test$CoapplicantIncome <- as.numeric(test$CoapplicantIncome)
```

# Data Exploration

The package includes a couple of friendly methods to explore the dataset:

```{r}
summarizeColumns(train)
```

disp is the measure of dispersion, for numerics and integers standard deviation is used, for categorical columns the qualitative variation.

```{r}
summarizeLevels(train[,-1]) 
```

# Preprocessing

## Filter outliers

```{r, Feature Engineering}

thr <- 40000

train <- capLargeValues(train, cols = c("ApplicantIncome"),threshold = thr)
test <- capLargeValues(test, cols = c("ApplicantIncome"),threshold = thr)

```

## Leveraging Caret

MLR offers the possibility to call Caret preprocess methods through the  makePreprocWrapperCaret method.

```{r, Leveraging Caret}

makePreprocWrapperCaret(learner, ppc.knnImpute = TRUE, ppc.pca = TRUE, ppc.pcaComp = 10)

```


# Missing Value Imputation

## Simple methods

```{r}

impSimple <- impute(train, 
              cols = list(Self_Employed = imputeMode(), LoanAmount = imputeMean()), 
              dummy.cols = c("Self_Employed","LoanAmount"), 
              dummy.type = "numeric")

summarizeColumns(impSimple$data)

```

## Type based

We can also impute all variables depending on their type:

```{r}
impType <- impute(train, 
                  classes = list(factor = imputeMode(), integer = imputeMean()),
                  dummy.classes = c("integer","factor"), dummy.type = "numeric")

summarizeColumns(impType$data)
```

Careful, dummy is integer, unexpected result...use as.logical

```{r}

unique(impType$data$LoanAmount[as.logical(impType$data$LoanAmount.dummy)])

testType<-reimpute(test,impType$desc)

unique(testType$LoanAmount[is.na(test$LoanAmount)])

```

## Using a learner

Imputation can also be done using any of the supported learners:

```{r}
impRPart <- impute(train[,-1], 
                    target = "Loan_Status",
                    cols = list(LoanAmount = imputeLearner(makeLearner("regr.rpart"))
                               ,Self_Employed = imputeLearner(makeLearner("classif.rpart"))),
                    dummy.cols = c("Self_Employed","LoanAmount"),
                    dummy.type = "numeric")
```


We can compare the results of the two imputation methods:

```{r}

cmpImputations <- data.frame(select(impRPart$data, LoanAmount, LoanAmount.dummy), select(impSimple$data, LoanAmount))

ggplot(filter(cmpImputations,LoanAmount.dummy==T)) + geom_point(aes(x=LoanAmount, y=LoanAmount.1))

```

## Custom Learners

Custom Imputation methods can be written based on MakeImputeMethod. An example can be found in [the package tutorials](https://mlr-org.github.io/mlr-tutorial/release/html/create_imputation/index.html) but it is not a good example as it uses LOCF (Last Observation Carried Forward) what is not a good method if you have train and test.

Instead I implemented mean by category

```{r}

imputeByCategory <- function( feature ) {
  
  makeImputeMethod(learn = function(data, target, col, ...) {
    input_list <- list(...)
    var <- input_list[["var"]]
    values <- data %>% 
      select_( var, col) %>% 
      group_by_(var) %>% 
      summarise_(.dots=setNames(paste0('mean(',col,',na.rm=T)'),
                                paste0('mean_',col)))
    head(values)
    return(list(values=values, var=var))
  }, 
  impute= function(data, target, col, values, var) {
    
    x <- data[[col]] 
    ocol <- data[[var]]
    ed <- ocol[is.na(x)]
    
    nv <- sapply(ed, function(y) {  # this sapply is not needed...
      values %>% 
        filter_(paste0(var,' == "',y,'"') ) %>% 
        select_(paste0('mean_',col)) %>% 
        unlist %>% 
        unname } ) 
    
    replace(x,is.na(x), nv)
    
  }, 
  args  = list(var=feature)
  )
}

impCustom <- impute(train, cols=list(LoanAmount=imputeByCategory("Education")), 
                    dummy.cols = c("LoanAmount"), dummy.type = "numeric")

```

```{r}
unique(impCustom$data$LoanAmount[as.logical(impCustom$data$LoanAmount.dummy)])

testCustom <- reimpute(test, impCustom$desc)

unique(testCustom$LoanAmount[is.na(test$LoanAmount)])
```

## Applying the imputation

We make the imputation final and apply the same imputation also to the test data:

```{r}
train <- imp$data

test <- reimpute(test, imp$desc) # this contains the imputation algorithm

summarizeColumns(test)
```

#Feature Selection

It is also possible to normalize, drop and remove constant features in the data or, as we will see later, in the learning task.

The main part is creating a Task that will contain the data to be used:


```{r, Machine Learning Task}
#Create a task indicating the positive class.
trainTask <- makeClassifTask(data = train, target = "Loan_Status", positive = "Y")

test$Loan_Status <- as.factor(0)
testTask <- makeClassifTask(data = test, target = "Loan_Status")

```


FeatSelControl: https://www.rdocumentation.org/packages/mlr/versions/2.10/topics/FeatSelControl

Features can also be removed from the task, they are not used in training but remain in the data:

```{r, Drop Features}
 
trainTask <- dropFeatures(task = trainTask, features = c("Loan_ID","Married.dummy"))

```

Normalize Features:

```{r, Normalize}

trainTask <- normalizeFeatures(trainTask, method = "standardize")
testTask <- normalizeFeatures(testTask, method = "standardize")

```


# Feature Importance

```{r, Variable Importance}

im_feat <- generateFilterValuesData(trainTask, method = c("information.gain"))
plotFilterValues(im_feat,n.show = 10)

```

# Parallelization

Costly operations can be parallelized, using the parallelMap package:

```{r, Parallel}
parallelStart(mode="socket", cpus=4, level= "mlr.tuneParams") # oner or all 
# "mlr.benchmark","mlr.resample","mlr.selectFeatures","mlr.tuneParams","mlr.ensemble"))
parallelGetRegisteredLevels()
```

```{r}

parallelGetRegisteredLevels()

```


# Learners

```{r, Learners List}

lstLearners<-listLearners("classif")
print("")
names(lstLearners)
```

```{r}
setDT(lstLearners)
lstLearners
```

## Learning about the parameters

```{r, Training RandomForest}

getParamSet("classif.randomForest")

```


```{r, RandomForest Learner}

rf <- makeLearner("classif.randomForest", predict.type = "prob", par.vals = list(ntree = 200, mtry = 3))

```

Define the parameters to be tuned, generating both continuous and discrete search spaces:

```{r}
#set tunable parameters
#grid search to find hyperparameters

rf_param <- makeParamSet( makeIntegerParam("ntree", lower = 50, upper = 500),
                          makeDiscreteParam("mtry", values=c(3, 4, 7, 10)),
                          makeIntegerParam("nodesize", lower = 10, upper = 50))
```

How the search is going to be made (other methods include GridSearch, SimulateadAnnealing, BayesianOptimization ):

```{r}
#let's do random search for 50 iterations
rancontrol <- makeTuneControlRandom(maxit = 50L)
```

Resampling strategy (other methods include repeatCV, bootstrap, holdout, leave-one-out, subsampling )

```{r}
#set 3 fold cross validation
set_cv <- makeResampleDesc("CV",iters = 3L)
```

Finally, we run the parameter tuning:

```{r}

rf_tune <- tuneParams(learner = rf, 
                      resampling = set_cv, 
                      task = trainTask, 
                      par.set = rf_param, 
                      control = rancontrol, 
                      measures = acc)

```

```{r}
# Stop parallel execution
parallelStop()
```

```{r}
#cv accuracy
rf_tune$y
```

```{r}
#best parameters
rf_tune$x
```

The effect of the parameters can be visualized pairwise, even when we have used more than two params. A learner is specified to "extrapolate":

```{r, Params dependence}
data <- generateHyperParsEffectData(rf_tune, partial.dep = T)

plt <- plotHyperParsEffect(data, x = "ntree", y = "nodesize", z = "acc.test.mean", plot.type = "heatmap",  partial.dep.learn = "regr.randomForest")
min_plt = min(data$data$acc.test.mean, na.rm = TRUE)
max_plt = max(data$data$acc.test.mean, na.rm = TRUE)
med_plt = mean(c(min_plt, max_plt))
plt + scale_fill_gradient2(breaks = seq(min_plt, max_plt, length.out = 5),
  low = "blue", mid = "white", high = "red", midpoint = med_plt)

```

The best params are then used to build the final model and calculate predictions:

```{r}
#using hyperparameters for modeling
rf.tree <- setHyperPars(rf, par.vals = rf_tune$x)

#train a model
rforest <- train(rf.tree, trainTask)
getLearnerModel(rforest)




```

```{r, ROC Curve Analysis}

oob_preds <- getOOBPreds(rforest, trainTask)

performance(oob_preds, auc)
```

```{r}
df <- generateThreshVsPerfData(oob_preds, measures = list(fpr, tpr, mmce, npv))
plotThreshVsPerf(df)
```

```{r}

df <- generateThreshVsPerfData(list(train = oob_preds), measures = list(fpr, tpr))
plotROCCurves(df)

```

Make final predictions  on the test data set:

```{r}

rfmodel <- predict(rforest, testTask)

```

compared to Caret, I would say is more structured, particularly  the visualization  parts.


mlrHyperOpt

https://mlr-org.github.io/Parameter-tuning-with-mlrHyperopt/

